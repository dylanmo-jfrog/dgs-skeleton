name: Teardown AWS Resources

on:
  workflow_dispatch:
    inputs:
      destroy_level:
        description: 'Destruction level'
        required: true
        type: choice
        options:
          - 'infrastructure-only'
          - 'complete-destroy'
        default: 'infrastructure-only'
      confirm_destroy:
        description: 'Type "destroy" to confirm you want to destroy resources'
        required: true
        type: string

jobs:
  teardown:
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read

    env:
      AWS_REGION: us-east-1
      EKS_CLUSTER_NAME: dgs-skeleton-cluster

    steps:
    - name: Verify confirmation
      if: github.event.inputs.confirm_destroy != 'destroy'
      run: |
        echo "You must type 'destroy' to confirm resource deletion"
        exit 1

    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Update kube config
      continue-on-error: true
      run: |
        echo "Attempting to update kubeconfig (may fail if cluster doesn't exist)..."
        aws eks update-kubeconfig --name $EKS_CLUSTER_NAME --region ${{ env.AWS_REGION }} || echo "Cluster may not exist, continuing..."

    - name: Remove Kubernetes Resources
      continue-on-error: true
      run: |
        echo "=== Removing all Kubernetes resources ==="
        
        # Delete specific deployment and service first
        kubectl delete deployment dgs-skeleton --timeout=60s || true
        kubectl delete service dgs-skeleton-service --timeout=120s || true
        kubectl delete secret artifactory-pull-secret || true
        
        # Clean up any remaining LoadBalancer services (this is critical!)
        echo "Cleaning up all LoadBalancer services..."
        kubectl get services -o jsonpath='{.items[?(@.spec.type=="LoadBalancer")].metadata.name}' | xargs -r -I {} kubectl delete service {} --timeout=120s || true
        
        # Clean up any remaining deployments, services, pods
        echo "Cleaning up remaining resources..."
        kubectl delete deployments --all --timeout=60s || true
        kubectl delete services --all --timeout=120s || true
        kubectl delete pods --all --timeout=60s || true
        kubectl delete secrets --all || true
        
        # Show remaining resources for debugging
        echo "=== Remaining Kubernetes resources (should be minimal) ==="
        kubectl get all --all-namespaces | grep -v "kubernetes\|kube-system\|default" || echo "No custom resources found"
        
        # Wait longer for AWS LoadBalancers to cleanup (they can take 2-3 minutes)
        echo "â³ Waiting 120 seconds for AWS LoadBalancer cleanup (ELB/NLB deletion)..."
        sleep 120
        
        # Verify LoadBalancer cleanup progress
        echo "Checking LoadBalancer cleanup status..."
        kubectl get services -A | grep LoadBalancer || echo "âœ… No LoadBalancer services found"

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v2
      with:
        terraform_version: 1.5.0

    - name: Terraform Init
      working-directory: ./terraform
      run: terraform init

    - name: Terraform Destroy
      working-directory: ./terraform
      run: terraform destroy -auto-approve

    - name: Manual cleanup of any remaining AWS resources
      continue-on-error: true
      run: |
        echo "=== Manual cleanup of potentially orphaned resources ==="
        
        # Sometimes LoadBalancers don't get cleaned up if they were created after Terraform
        # Check for and manually delete any remaining LoadBalancers
        echo "Checking for orphaned LoadBalancers..."
        
        # Clean up any remaining ALBs/NLBs that might have dgs-skeleton in the name or tags
        aws elbv2 describe-load-balancers --region ${{ env.AWS_REGION }} --query 'LoadBalancers[?contains(LoadBalancerName, `k8s-default-dgsskele`) || contains(LoadBalancerName, `k8s-`) ].LoadBalancerArn' --output text | while read -r lb_arn; do
          if [ -n "$lb_arn" ]; then
            echo "Deleting orphaned LoadBalancer: $lb_arn"
            aws elbv2 delete-load-balancer --load-balancer-arn "$lb_arn" --region ${{ env.AWS_REGION }} || true
          fi
        done
        
        # Clean up security groups that might be left behind (retry after LB deletion)
        echo "Waiting 60 seconds for LoadBalancer deletion to propagate..."
        sleep 60
        
        echo "Checking for orphaned security groups..."
        aws ec2 describe-security-groups --region ${{ env.AWS_REGION }} --filters "Name=group-name,Values=k8s-elb-*" --query 'SecurityGroups[].GroupId' --output text | while read -r sg_id; do
          if [ -n "$sg_id" ]; then
            echo "Attempting to delete security group: $sg_id"
            aws ec2 delete-security-group --group-id "$sg_id" --region ${{ env.AWS_REGION }} || true
          fi
        done

    - name: Cleanup Local Kubeconfig
      if: always()
      run: |
        rm -f ~/.kube/config || true

    - name: Cleanup Backend Infrastructure
      if: github.event.inputs.destroy_level == 'complete-destroy'
      working-directory: ./terraform/bootstrap
      run: |
        echo "ğŸš¨ COMPLETE DESTROY: Removing S3 state bucket..."
        echo "âš ï¸  After this, you'll need to re-run bootstrap to deploy again!"
        terraform init
        terraform destroy -auto-approve

    - name: Preserve State Message
      if: github.event.inputs.destroy_level == 'infrastructure-only'
      run: |
        echo "âœ… Infrastructure destroyed but S3 state bucket preserved"
        echo "ğŸ’¡ You can re-deploy anytime with workflow.yml"
        echo "ğŸ—„ï¸  State bucket 'dgs-skeleton-terraform-state' remains for future deployments"

    - name: Verify All Resources Destroyed
      run: |
        echo "=== COMPREHENSIVE RESOURCE VERIFICATION ==="
        
        # Check for any remaining EKS clusters
        echo "ğŸ” Checking for EKS clusters..."
        CLUSTERS=$(aws eks list-clusters --region ${{ env.AWS_REGION }} --query 'clusters[?contains(@, `dgs-skeleton`)]' --output text)
        if [ -n "$CLUSTERS" ]; then
          echo "âŒ Found remaining EKS clusters: $CLUSTERS"
        else
          echo "âœ… No EKS clusters found"
        fi
        
        # Check for VPCs
        echo "ğŸ” Checking for VPCs..."
        VPCS=$(aws ec2 describe-vpcs --region ${{ env.AWS_REGION }} --filters "Name=tag:Name,Values=*dgs-skeleton*" --query 'Vpcs[].VpcId' --output text)
        if [ -n "$VPCS" ]; then
          echo "âŒ Found remaining VPCs: $VPCS"
        else
          echo "âœ… No tagged VPCs found"
        fi
        
        # Check for Classic Load Balancers
        echo "ğŸ” Checking for Classic Load Balancers..."
        CLBS=$(aws elb describe-load-balancers --region ${{ env.AWS_REGION }} --query 'LoadBalancerDescriptions[?contains(LoadBalancerName, `dgs-skeleton`)].LoadBalancerName' --output text)
        if [ -n "$CLBS" ]; then
          echo "âŒ Found Classic Load Balancers: $CLBS"
        else
          echo "âœ… No Classic Load Balancers found"
        fi
        
        # Check for Application/Network Load Balancers
        echo "ğŸ” Checking for ALB/NLB Load Balancers..."
        ALBS=$(aws elbv2 describe-load-balancers --region ${{ env.AWS_REGION }} --query 'LoadBalancers[?contains(LoadBalancerName, `dgs-skeleton`) || contains(tags, `dgs-skeleton`)].LoadBalancerName' --output text 2>/dev/null || echo "")
        if [ -n "$ALBS" ]; then
          echo "âŒ Found ALB/NLB Load Balancers: $ALBS"
        else
          echo "âœ… No ALB/NLB Load Balancers found"
        fi
        
        # Check for Security Groups
        echo "ğŸ” Checking for Security Groups..."
        SGS=$(aws ec2 describe-security-groups --region ${{ env.AWS_REGION }} --filters "Name=tag:Name,Values=*dgs-skeleton*" --query 'SecurityGroups[].GroupId' --output text)
        if [ -n "$SGS" ]; then
          echo "âŒ Found Security Groups: $SGS"
        else
          echo "âœ… No tagged Security Groups found"
        fi
        
        # Check for EC2 instances
        echo "ğŸ” Checking for EC2 instances..."
        INSTANCES=$(aws ec2 describe-instances --region ${{ env.AWS_REGION }} --filters "Name=tag:Name,Values=*dgs-skeleton*" "Name=instance-state-name,Values=running,stopped,stopping,pending" --query 'Reservations[].Instances[].InstanceId' --output text)
        if [ -n "$INSTANCES" ]; then
          echo "âŒ Found EC2 instances: $INSTANCES"
        else
          echo "âœ… No tagged EC2 instances found"
        fi
        
        # Check for NAT Gateways
        echo "ğŸ” Checking for NAT Gateways..."
        NATS=$(aws ec2 describe-nat-gateways --region ${{ env.AWS_REGION }} --filter "Name=tag:Name,Values=*dgs-skeleton*" --query 'NatGateways[?State!=`deleted`].NatGatewayId' --output text)
        if [ -n "$NATS" ]; then
          echo "âŒ Found NAT Gateways: $NATS"
        else
          echo "âœ… No NAT Gateways found"
        fi
        
        # Check for S3 buckets (based on destroy level)
        if [ "${{ github.event.inputs.destroy_level }}" = "complete-destroy" ]; then
          echo "ğŸ” Checking for S3 buckets..."
          BUCKETS=$(aws s3api list-buckets --query 'Buckets[?contains(Name, `dgs-skeleton`)].Name' --output text)
          if [ -n "$BUCKETS" ]; then
            echo "âŒ Found S3 buckets: $BUCKETS"
          else
            echo "âœ… No S3 buckets found"
          fi
        else
          echo "ğŸ’¾ S3 state bucket preserved for future deployments"
        fi
        
        echo ""
        echo "=== CLEANUP SUMMARY ==="
        if [ "${{ github.event.inputs.destroy_level }}" = "complete-destroy" ]; then
          echo "ğŸ—‘ï¸  COMPLETE DESTROY executed"
          echo "ğŸ’¡ To redeploy: Run terraform/bootstrap first, then workflow.yml"
        else
          echo "ğŸ—ï¸  INFRASTRUCTURE-ONLY destroy executed"
          echo "ğŸ’¡ To redeploy: Just run workflow.yml (bootstrap not needed)"
        fi
        echo "=== VERIFICATION COMPLETE ==="
