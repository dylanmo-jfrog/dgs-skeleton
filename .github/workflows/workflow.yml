name: "Build and Publish DGS"
on: 
  workflow_dispatch: # manual trigger
    inputs:
      deploy_to_eks:
        description: 'Deploy to EKS?'
        type: choice
        options:
          - 'yes'
          - 'no'
        required: true
        default: 'no'
  push:
    branches:
      - main
    paths:
      - '**.java'
      - 'pom.xml'

jobs:
  build:
    runs-on: ubuntu-latest

    # Setup environment variables, some of them can be overridden by repository variables
    permissions:
      id-token: write
      contents: write
      pull-requests: write
      security-events: write
      actions: read 
      attestations: write    
      packages: write    
    env:
      JF_HOST: ${{ vars.JF_HOST                 || 'soleng.jfrog.io' }}
      JF_PROJECT_KEY: ${{ vars.JF_PROJECT_KEY   || 'dmodgs' }}
      BUILD_NAME: ${{ vars.BUILD_NAME           || 'dylanmo-dgs-docker-build' }}
      BUILD_NUMBER: ${{ vars.BUILD_NUMBER       || github.run_id }}
      DOCKER_REPO: 'dylanmo-dev-docker-local'
      IMAGE_NAME: 'graphql-dgs-tcat'
      MVN_VIRTUAL_REPO_SNAPSHOT: 'dylanmo-dev-libs-snapshot'
      MVN_VIRTUAL_REPO_RELEASE: 'dylanmo-dev-libs-release'
    
    outputs:
      build-number: ${{ env.BUILD_NUMBER }}
      pkg-version: 1.${{ env.BUILD_NUMBER }}.0

    # Here we install all the tools : docker buildx, QEMU, JDK 11, JFrog CLI
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set up JDK 11
        uses: actions/setup-java@v3
        with:
          java-version: '11'
          distribution: 'corretto'        
          cache: 'maven'
    
    # Install JFrog CLI using the JFrog CLI GitHub Action      
      - name: Setup JFrog CLI
        id: setup-cli
        uses: jfrog/setup-jfrog-cli@v4
        env:
          JF_URL: https://${{ env.JF_HOST }}/
          JF_PROJECT: ${{ env.JF_PROJECT_KEY }}
          # Override any automatic build number with our desired one
          JFROG_CLI_BUILD_NAME: ${{ env.BUILD_NAME }}
          JFROG_CLI_BUILD_NUMBER: ${{ env.BUILD_NUMBER }}
        with:
            oidc-provider-name: dylanmo-github
            oidc-audience: dylanmo-github

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
     
      # We build & test the WAR file, scan it and if scan result ok, we publish it to our artifactory maven repository
      - name: Run Maven build
        id: build 
        env:
          PKG_VERSION: 1.${{ env.BUILD_NUMBER }}.0
          JF_PROJECT: ${{ env.JF_PROJECT_KEY }}
        run: |
            # Collect environment variables for the build
            jf rt bce ${{ env.BUILD_NAME }} ${{ env.BUILD_NUMBER }} 
            jf mvnc --repo-resolve-releases=${{ env.MVN_VIRTUAL_REPO_RELEASE }} \
              --repo-deploy-releases=${{ env.MVN_VIRTUAL_REPO_RELEASE }} \
              --repo-resolve-snapshots=${{ env.MVN_VIRTUAL_REPO_SNAPSHOT }} \
              --repo-deploy-snapshots=${{ env.MVN_VIRTUAL_REPO_SNAPSHOT }}

            # Run curation audit and print output to log
            audit_output=$(jf ca 2>&1)

            # Check if blocked packages were found
            if echo "$audit_output" | grep -q "Found [1-9][0-9]* blocked packages"; then
              echo "::error::Curation audit detected blocked packages - failing build"
              echo "curation_failed=true" >> $GITHUB_OUTPUT
              exit 1
            fi

            jf rt bag ${{ env.BUILD_NAME }} ${{ env.BUILD_NUMBER }} # collect vcs info
            jf mvn -f pom.xml -Drevision=${{ env.PKG_VERSION }} -Dmaven.test.skip=true clean package # build the war
            jf audit --mvn --fail=false --vuln # scan the war
            jf mvn -f pom.xml -Drevision=${{ env.PKG_VERSION }} -Dmaven.test.skip=true --build-name=${{ env.BUILD_NAME }} --build-number=${{ env.BUILD_NUMBER }} deploy # deploy the war
            jf rt bp ${{ env.BUILD_NAME }} ${{ env.BUILD_NUMBER }} # publish the build info
            jf bs --fail=false --vuln ${{ env.BUILD_NAME }} ${{ env.BUILD_NUMBER }}
            
            # Debug: List files in target directory
            echo "Listing files in target directory:"
            ls -la target/
            
            # Debug: Check if WAR file exists with exact name
            echo "Checking for WAR file:"
            ls -la target/dgs-skeleton-webapp-${{ env.PKG_VERSION }}.war || echo "WAR file not found!"

     # If the curation audit failed, we skip the docker build and push steps       
      - name: Authenticate Docker
        if: steps.build.outputs.curation_failed != 'true'
        uses: docker/login-action@v3
        with:
          registry: ${{ env.JF_HOST }}
          username: ${{ steps.setup-cli.outputs.oidc-user }}
          password: ${{ steps.setup-cli.outputs.oidc-token }}
                
      - name: Build and push docker tomcat + war based docker images
        if: steps.build.outputs.curation_failed != 'true'
        env: 
          PKG_VERSION: 1.${{ env.BUILD_NUMBER }}.0
          JF_PROJECT: ${{ env.JF_PROJECT_KEY }}
        uses: docker/build-push-action@v6
        id: docker_build
        with:
          context: .
          platforms: linux/amd64,linux/arm64
          provenance: true
          push: true
          build-args: |
            WAR_FILE_NAME=dgs-skeleton-webapp-${{ env.PKG_VERSION }}.war
          tags: ${{ env.JF_HOST }}/${{ env.DOCKER_REPO }}/${{ env.IMAGE_NAME}}:${{ env.PKG_VERSION }}
      
      - name: Push Docker build info
        env:
          PKG_VERSION: 1.${{ env.BUILD_NUMBER }}.0
          JF_PROJECT: ${{ env.JF_PROJECT_KEY }}
        run: |
          echo ${{ env.JF_HOST }}/${{ env.DOCKER_REPO }}/${{ env.IMAGE_NAME}}:${{ env.PKG_VERSION }}@${{ steps.docker_build.outputs.digest }} > ./image-metadata.json
          # Collect environment variables for the build
          jf rt bce ${{ env.BUILD_NAME }} ${{ env.BUILD_NUMBER }} 
          # collect vcs info
          jf rt bag ${{ env.BUILD_NAME }} ${{ env.BUILD_NUMBER }}
          # As the docker images have been built by buildx, we don't have them locally, 
          # so we link the build info to the already published images in previous steps
          jf rt bdc ${{ env.DOCKER_REPO }} --image-file ./image-metadata.json --build-name ${{ env.BUILD_NAME }} --build-number ${{ env.BUILD_NUMBER }}
          # Publish build info
          jf rt bp ${{ env.BUILD_NAME }} ${{ env.BUILD_NUMBER }}

      - name: Scan the build & the docker image
        env:
          PKG_VERSION: 1.${{ env.BUILD_NUMBER }}.0
          JF_PROJECT: ${{ env.JF_PROJECT_KEY }}
        run: |
          # Xray build scan example
          docker pull ${{ env.JF_HOST }}/${{ env.DOCKER_REPO }}/${{ env.IMAGE_NAME}}:${{ env.PKG_VERSION }}
          # jf docker scan ${{ env.JF_HOST }}/${{ env.DOCKER_REPO }}/${{ env.IMAGE_NAME}}:${{ env.PKG_VERSION }} --severity='HIGH,CRITICAL' --vuln
          # jf bs --fail=false --vuln ${{ env.BUILD_NAME }} ${{ env.BUILD_NUMBER }}

  deploy-to-eks:
    needs: build
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.deploy_to_eks == 'yes'
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
      pull-requests: read

    env:
      AWS_REGION: us-east-1
      EKS_CLUSTER_NAME: dgs-skeleton-cluster
      JF_HOST: ${{ vars.JF_HOST || 'soleng.jfrog.io' }}
      JF_PROJECT_KEY: ${{ vars.JF_PROJECT_KEY || 'dmodgs' }}
      BUILD_NUMBER: ${{ needs.build.outputs.build-number }}
      PKG_VERSION: ${{ needs.build.outputs.pkg-version }}
      DOCKER_REPO: 'dylanmo-dev-docker-local'
      IMAGE_NAME: 'graphql-dgs-tcat'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup JFrog CLI
      id: setup-cli
      uses: jfrog/setup-jfrog-cli@v4
      env:
        JF_URL: https://${{ env.JF_HOST }}/
        JF_PROJECT: ${{ env.JF_PROJECT_KEY }}
      with:
        oidc-provider-name: dylanmo-github
        oidc-audience: dylanmo-github

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v2
      with:
        terraform_version: 1.5.0

    - name: Setup Backend Infrastructure
      working-directory: ./terraform/bootstrap
      run: |
        terraform init
        
        # Try to import existing S3 bucket
        terraform import aws_s3_bucket.terraform_state dgs-skeleton-terraform-state || true
        
        # Apply changes (this will update any configuration if needed)
        terraform apply -auto-approve

    - name: Terraform Init
      working-directory: ./terraform
      run: terraform init

    - name: Terraform Apply
      working-directory: ./terraform
      run: terraform apply -auto-approve

    - name: Update kube config
      run: aws eks update-kubeconfig --name $EKS_CLUSTER_NAME --region ${{ env.AWS_REGION }}

    - name: Deploy to EKS
      run: |
        # Wait a moment for EKS cluster to be fully ready after Terraform apply
        echo "Waiting for cluster to be fully ready..."
        sleep 30
        
        # Verify cluster connectivity
        kubectl cluster-info
        
        # Replace the image reference in the deployment file with the Artifactory image
        FULL_IMAGE_NAME="${{ env.JF_HOST }}/${{ env.DOCKER_REPO }}/${{ env.IMAGE_NAME }}:${{ env.PKG_VERSION }}"
        echo "=== Using image: $FULL_IMAGE_NAME ==="
        
        # Verify the image exists in Artifactory before deploying
        echo "=== Verifying image exists in Artifactory ==="
        echo "OIDC User: ${{ steps.setup-cli.outputs.oidc-user }}"
        echo "Image tag: ${{ env.PKG_VERSION }}"
        
        # Check if image exists using JFrog CLI
        if jf rt search "${{ env.DOCKER_REPO }}/${{ env.IMAGE_NAME }}/${{ env.PKG_VERSION }}/"; then
          echo "‚úÖ Image found in Artifactory"
        else
          echo "‚ùå Image not found in Artifactory. Available images:"
          jf rt search "${{ env.DOCKER_REPO }}/${{ env.IMAGE_NAME }}/" || echo "No images found for ${{ env.IMAGE_NAME }}"
          exit 1
        fi
        
        sed -i "s|image: CONTAINER_IMAGE|image: ${FULL_IMAGE_NAME}|" k8s/deployment.yaml
        
        # Show the final deployment file
        echo "=== Final deployment.yaml ==="
        cat k8s/deployment.yaml
        
        # Create image pull secret for Artifactory using OIDC token
        echo "=== Creating image pull secret ==="
        echo "Docker server: ${{ env.JF_HOST }}"
        echo "Docker username: ${{ steps.setup-cli.outputs.oidc-user }}"
        
        kubectl create secret docker-registry artifactory-pull-secret \
          --docker-server=${{ env.JF_HOST }} \
          --docker-username=${{ steps.setup-cli.outputs.oidc-user }} \
          --docker-password=${{ steps.setup-cli.outputs.oidc-token }} \
          --dry-run=client -o yaml | kubectl apply -f -
          
        # Verify the secret was created and test authentication
        kubectl get secret artifactory-pull-secret
        
        # Test if we can pull the image manually (this will help debug auth issues)
        echo "=== Testing image pull authentication ==="
        if docker login ${{ env.JF_HOST }} -u ${{ steps.setup-cli.outputs.oidc-user }} -p ${{ steps.setup-cli.outputs.oidc-token }}; then
          echo "‚úÖ Docker login successful"
          # Try to pull the image to verify it works
          if docker pull $FULL_IMAGE_NAME; then
            echo "‚úÖ Image pull successful"
          else
            echo "‚ùå Image pull failed"
            exit 1
          fi
        else
          echo "‚ùå Docker login failed"
          exit 1
        fi
        
        # Apply the deployment with validation disabled if needed
        kubectl apply -f k8s/deployment.yaml --validate=false
        
        # Show what was created
        echo "=== Checking deployment status ==="
        kubectl get pods -l app=dgs-skeleton
        kubectl get deployment dgs-skeleton
        kubectl get service dgs-skeleton-service
        
        # Wait for deployment to be ready with shorter timeout and better debugging
        echo "=== Waiting for deployment (max 3 minutes) ==="
        if ! kubectl rollout status deployment/dgs-skeleton --timeout=180s; then
          echo "‚ùå Deployment failed or timed out. Debugging..."
          
          echo "=== Pod Status ==="
          kubectl get pods -l app=dgs-skeleton -o wide
          
          echo "=== Pod Events ==="
          kubectl get events --sort-by=.metadata.creationTimestamp | grep dgs-skeleton
          
          echo "=== Pod Descriptions ==="
          kubectl describe pods -l app=dgs-skeleton
          
          echo "=== Pod Logs ==="
          for pod in $(kubectl get pods -l app=dgs-skeleton -o jsonpath='{.items[*].metadata.name}'); do
            echo "--- Logs for $pod ---"
            kubectl logs $pod --tail=50 || echo "No logs available for $pod"
          done
          
          echo "=== Deployment Description ==="
          kubectl describe deployment dgs-skeleton
          
          echo "=== Image Pull Secret ==="
          kubectl get secret artifactory-pull-secret -o yaml
          
          exit 1
        fi
        
        # Show access information
        echo "=== Deployment Complete ==="
        kubectl get pods -l app=dgs-skeleton
        kubectl get service dgs-skeleton-service
        
        # Get node IPs and service port for access
        echo "=== Access Information ==="
        NODE_IPS=$(kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}')
        SERVICE_PORT=$(kubectl get service dgs-skeleton-service -o jsonpath='{.spec.ports[0].nodePort}')
        
        # Verify pods are ready through Kubernetes API instead of external access
        echo "=== Verifying pods are ready internally ==="
        kubectl get pods -l app=dgs-skeleton -o wide
        
        # Wait for all pods to be in Ready state
        echo "Waiting for all pods to be ready..."
        kubectl wait --for=condition=Ready pod -l app=dgs-skeleton --timeout=300s
        
        # Test health check internally using port-forward (more reliable than NodePort)
        echo "Testing health check via port-forward..."
        kubectl port-forward service/dgs-skeleton-service 8080:80 &
        PORT_FORWARD_PID=$!
        sleep 10
        
        # Try health check with port-forward
        for i in {1..10}; do
          if curl -f -s "http://localhost:8080/jfrog-demo/health/status" > /dev/null 2>&1; then
            echo "‚úÖ Health check passed! Application is ready."
            kill $PORT_FORWARD_PID
            
            # Show access information
            if [ -n "$NODE_IPS" ] && [ -n "$SERVICE_PORT" ]; then
              for ip in $NODE_IPS; do
                echo "üöÄ GraphQL API should be available at: http://$ip:$SERVICE_PORT/jfrog-demo/graphql"
                echo "üîç Health check endpoint: http://$ip:$SERVICE_PORT/jfrog-demo/health/status"
              done
            else
              echo "üöÄ Use port-forwarding to access: kubectl port-forward service/dgs-skeleton-service 8080:80"
              echo "üîç Then access GraphQL at: http://localhost:8080/jfrog-demo/graphql"
            fi
            
            # Success - exit the deployment step successfully
            exit 0
          else
            echo "‚è≥ Attempt $i/10: Health check not ready yet, waiting 10 seconds..."
            sleep 10
          fi
          
          if [ $i -eq 10 ]; then
            echo "‚ùå Health check failed after port-forward attempts. Check pod logs:"
            kubectl logs -l app=dgs-skeleton --tail=50
            kill $PORT_FORWARD_PID
            exit 1
          fi
        done